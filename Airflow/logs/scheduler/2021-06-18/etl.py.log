[2021-06-18 10:01:35,421] {scheduler_job.py:182} INFO - Started process (PID=2607) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:01:35,422] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:01:35,422] {logging_mixin.py:104} INFO - [2021-06-18 10:01:35,422] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:01:35,423] {scheduler_job.py:645} WARNING - No viable dags retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:01:35,435] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.019 seconds
[2021-06-18 10:02:05,716] {scheduler_job.py:182} INFO - Started process (PID=2704) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:02:05,718] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:02:05,719] {logging_mixin.py:104} INFO - [2021-06-18 10:02:05,718] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:02:05,721] {scheduler_job.py:645} WARNING - No viable dags retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:02:05,729] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.017 seconds
[2021-06-18 10:02:36,183] {scheduler_job.py:182} INFO - Started process (PID=2779) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:02:36,184] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:02:36,185] {logging_mixin.py:104} INFO - [2021-06-18 10:02:36,185] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:02:36,187] {scheduler_job.py:645} WARNING - No viable dags retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:02:36,199] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.019 seconds
[2021-06-18 10:03:06,308] {scheduler_job.py:182} INFO - Started process (PID=2875) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:03:06,309] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:03:06,310] {logging_mixin.py:104} INFO - [2021-06-18 10:03:06,310] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:03:06,311] {scheduler_job.py:645} WARNING - No viable dags retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:03:06,325] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.021 seconds
[2021-06-18 10:03:36,843] {scheduler_job.py:182} INFO - Started process (PID=2960) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:03:36,845] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:03:36,846] {logging_mixin.py:104} INFO - [2021-06-18 10:03:36,845] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:03:36,847] {scheduler_job.py:645} WARNING - No viable dags retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:03:36,857] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.018 seconds
[2021-06-18 10:04:07,123] {scheduler_job.py:182} INFO - Started process (PID=3046) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:04:07,124] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:04:07,125] {logging_mixin.py:104} INFO - [2021-06-18 10:04:07,125] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:04:07,126] {scheduler_job.py:645} WARNING - No viable dags retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:04:07,138] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.019 seconds
[2021-06-18 10:04:37,412] {scheduler_job.py:182} INFO - Started process (PID=3143) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:04:37,412] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:04:37,413] {logging_mixin.py:104} INFO - [2021-06-18 10:04:37,413] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:04:37,418] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:04:39,210] {logging_mixin.py:104} INFO - [2021-06-18 10:04:39,208] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:04:39,222] {logging_mixin.py:104} INFO - [2021-06-18 10:04:39,220] {dag.py:1851} INFO - Creating ORM DAG for etl_workflow
[2021-06-18 10:04:39,227] {logging_mixin.py:104} INFO - [2021-06-18 10:04:39,227] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:04:39,236] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 1.826 seconds
[2021-06-18 10:05:09,781] {scheduler_job.py:182} INFO - Started process (PID=3230) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:05:09,782] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:05:09,783] {logging_mixin.py:104} INFO - [2021-06-18 10:05:09,783] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:05:09,789] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:05:09,806] {logging_mixin.py:104} INFO - [2021-06-18 10:05:09,806] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:05:09,820] {logging_mixin.py:104} INFO - [2021-06-18 10:05:09,820] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:05:09,827] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.050 seconds
[2021-06-18 10:05:40,063] {scheduler_job.py:182} INFO - Started process (PID=3317) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:05:40,064] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:05:40,064] {logging_mixin.py:104} INFO - [2021-06-18 10:05:40,064] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:05:40,071] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:05:40,258] {logging_mixin.py:104} INFO - [2021-06-18 10:05:40,258] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:05:40,274] {logging_mixin.py:104} INFO - [2021-06-18 10:05:40,274] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:05:40,284] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.224 seconds
[2021-06-18 10:06:10,557] {scheduler_job.py:182} INFO - Started process (PID=3414) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:06:10,558] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:06:10,559] {logging_mixin.py:104} INFO - [2021-06-18 10:06:10,558] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:06:10,565] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:06:10,587] {logging_mixin.py:104} INFO - [2021-06-18 10:06:10,587] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:06:10,600] {logging_mixin.py:104} INFO - [2021-06-18 10:06:10,600] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:06:10,606] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.051 seconds
[2021-06-18 10:06:41,072] {scheduler_job.py:182} INFO - Started process (PID=3489) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:06:41,073] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:06:41,073] {logging_mixin.py:104} INFO - [2021-06-18 10:06:41,073] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:06:41,080] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:06:41,258] {logging_mixin.py:104} INFO - [2021-06-18 10:06:41,258] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:06:41,272] {logging_mixin.py:104} INFO - [2021-06-18 10:06:41,272] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:06:41,280] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.212 seconds
[2021-06-18 10:25:11,085] {scheduler_job.py:182} INFO - Started process (PID=35) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:25:11,086] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:25:11,086] {logging_mixin.py:104} INFO - [2021-06-18 10:25:11,086] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:25:11,099] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:25:15,568] {logging_mixin.py:104} INFO - [2021-06-18 10:25:15,568] {manager.py:548} ERROR - Creation of Permission View Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(etl_workflow) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, last_updated, dag_hash) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(last_updated)s, %(dag_hash)s)]
[parameters: {'dag_id': 'etl_workflow', 'fileloc': '/opt/airflow/dags/etl.py', 'fileloc_hash': 34118401899435808, 'data': '{"__version": 1, "dag": {"edge_info": {}, "_dag_id": "etl_workflow", "fileloc": "/opt/airflow/dags/etl.py", "_task_group": {"_group_id": null, "prefi ... (277 characters truncated) ... gmail.com", "start_date": {"__var": 1623974400.0, "__type": "datetime"}}, "__type": "dict"}, "timezone": "UTC", "tasks": [], "dag_dependencies": []}}', 'last_updated': datetime.datetime(2021, 6, 18, 10, 25, 11, 113667, tzinfo=Timezone('UTC')), 'dag_hash': '39e14c5876b5d37973c5cc577b65bfe0'}]
(Background on this error at: http://sqlalche.me/e/13/gkpj)
[2021-06-18 10:25:15,571] {logging_mixin.py:104} INFO - [2021-06-18 10:25:15,568] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:25:15,585] {logging_mixin.py:104} INFO - [2021-06-18 10:25:15,584] {dag.py:1851} INFO - Creating ORM DAG for etl_workflow
[2021-06-18 10:25:15,590] {logging_mixin.py:104} INFO - [2021-06-18 10:25:15,590] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:25:15,596] {scheduler_job.py:193} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(etl_workflow) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 187, in _run_file_processor
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 652, in process_file
    dagbag.sync_to_db()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 594, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.6/site-packages/tenacity/__init__.py", line 390, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.6/site-packages/tenacity/__init__.py", line 356, in iter
    return fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 608, in sync_to_db
    DAG.bulk_write_to_db(self.dags.values(), session=session)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 1919, in bulk_write_to_db
    DagCode.bulk_sync_to_db([dag.fileloc for dag in orm_dags])
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.6/contextlib.py", line 88, in __exit__
    next(self.gen)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 32, in create_session
    session.commit()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1046, in commit
    self.transaction.commit()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 504, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 483, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 1083, in _emit_insert_statements
    c = cached_connections[connection].execute(statement, multiparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(etl_workflow) already exists.

[SQL: INSERT INTO dag (dag_id, root_dag_id, is_paused, is_subdag, is_active, last_parsed_time, last_pickled, last_expired, scheduler_lock, pickle_id, fileloc, owners, description, default_view, schedule_interval, concurrency, has_task_concurrency_limits, next_dagrun, next_dagrun_create_after) VALUES (%(dag_id)s, %(root_dag_id)s, %(is_paused)s, %(is_subdag)s, %(is_active)s, %(last_parsed_time)s, %(last_pickled)s, %(last_expired)s, %(scheduler_lock)s, %(pickle_id)s, %(fileloc)s, %(owners)s, %(description)s, %(default_view)s, %(schedule_interval)s, %(concurrency)s, %(has_task_concurrency_limits)s, %(next_dagrun)s, %(next_dagrun_create_after)s)]
[parameters: {'dag_id': 'etl_workflow', 'root_dag_id': None, 'is_paused': True, 'is_subdag': False, 'is_active': True, 'last_parsed_time': datetime.datetime(2021, 6, 18, 10, 25, 15, 590672, tzinfo=Timezone('UTC')), 'last_pickled': None, 'last_expired': None, 'scheduler_lock': None, 'pickle_id': None, 'fileloc': '/opt/airflow/dags/etl.py', 'owners': '', 'description': None, 'default_view': 'tree', 'schedule_interval': '{"type": "timedelta", "attrs": {"days": 1, "seconds": 0, "microseconds": 0}}', 'concurrency': 16, 'has_task_concurrency_limits': False, 'next_dagrun': None, 'next_dagrun_create_after': None}]
(Background on this error at: http://sqlalche.me/e/13/gkpj)
[2021-06-18 10:25:45,716] {scheduler_job.py:182} INFO - Started process (PID=164) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:25:45,718] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:25:45,718] {logging_mixin.py:104} INFO - [2021-06-18 10:25:45,718] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:25:45,724] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:25:45,897] {logging_mixin.py:104} INFO - [2021-06-18 10:25:45,897] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:25:45,908] {logging_mixin.py:104} INFO - [2021-06-18 10:25:45,908] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:25:45,915] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.203 seconds
[2021-06-18 10:26:16,152] {scheduler_job.py:182} INFO - Started process (PID=239) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:26:16,153] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:26:16,154] {logging_mixin.py:104} INFO - [2021-06-18 10:26:16,154] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:26:16,160] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:26:16,180] {logging_mixin.py:104} INFO - [2021-06-18 10:26:16,179] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:26:16,192] {logging_mixin.py:104} INFO - [2021-06-18 10:26:16,192] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:26:16,199] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.050 seconds
[2021-06-18 10:26:46,480] {scheduler_job.py:182} INFO - Started process (PID=335) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:26:46,481] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:26:46,481] {logging_mixin.py:104} INFO - [2021-06-18 10:26:46,481] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:26:46,487] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:26:46,659] {logging_mixin.py:104} INFO - [2021-06-18 10:26:46,659] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:26:46,675] {logging_mixin.py:104} INFO - [2021-06-18 10:26:46,675] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:26:46,682] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.205 seconds
[2021-06-18 10:27:16,778] {scheduler_job.py:182} INFO - Started process (PID=431) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:27:16,779] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:27:16,779] {logging_mixin.py:104} INFO - [2021-06-18 10:27:16,779] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:27:16,787] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:27:16,806] {logging_mixin.py:104} INFO - [2021-06-18 10:27:16,806] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:27:16,819] {logging_mixin.py:104} INFO - [2021-06-18 10:27:16,819] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:27:16,826] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.051 seconds
[2021-06-18 10:27:47,017] {scheduler_job.py:182} INFO - Started process (PID=506) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:27:47,018] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:27:47,019] {logging_mixin.py:104} INFO - [2021-06-18 10:27:47,019] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:27:47,026] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:27:47,241] {logging_mixin.py:104} INFO - [2021-06-18 10:27:47,241] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:27:47,253] {logging_mixin.py:104} INFO - [2021-06-18 10:27:47,253] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:27:47,260] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.247 seconds
[2021-06-18 10:28:17,390] {scheduler_job.py:182} INFO - Started process (PID=604) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:28:17,391] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:28:17,392] {logging_mixin.py:104} INFO - [2021-06-18 10:28:17,392] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:28:17,398] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:28:17,417] {logging_mixin.py:104} INFO - [2021-06-18 10:28:17,416] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:28:17,429] {logging_mixin.py:104} INFO - [2021-06-18 10:28:17,429] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:28:17,435] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.051 seconds
[2021-06-18 10:28:47,731] {scheduler_job.py:182} INFO - Started process (PID=678) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:28:47,732] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:28:47,733] {logging_mixin.py:104} INFO - [2021-06-18 10:28:47,733] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:28:47,741] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:28:47,905] {logging_mixin.py:104} INFO - [2021-06-18 10:28:47,905] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:28:47,919] {logging_mixin.py:104} INFO - [2021-06-18 10:28:47,919] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:28:47,926] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.199 seconds
[2021-06-18 10:29:17,991] {scheduler_job.py:182} INFO - Started process (PID=775) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:29:17,992] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:29:17,992] {logging_mixin.py:104} INFO - [2021-06-18 10:29:17,992] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:29:17,999] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:29:18,015] {logging_mixin.py:104} INFO - [2021-06-18 10:29:18,014] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:29:18,029] {logging_mixin.py:104} INFO - [2021-06-18 10:29:18,029] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:29:18,036] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.049 seconds
[2021-06-18 10:29:48,208] {scheduler_job.py:182} INFO - Started process (PID=870) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:29:48,209] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:29:48,210] {logging_mixin.py:104} INFO - [2021-06-18 10:29:48,210] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:29:48,216] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:29:48,386] {logging_mixin.py:104} INFO - [2021-06-18 10:29:48,386] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:29:48,398] {logging_mixin.py:104} INFO - [2021-06-18 10:29:48,398] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:29:48,406] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.202 seconds
[2021-06-18 10:30:18,600] {scheduler_job.py:182} INFO - Started process (PID=945) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:30:18,601] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:30:18,602] {logging_mixin.py:104} INFO - [2021-06-18 10:30:18,602] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:30:18,610] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:30:18,629] {logging_mixin.py:104} INFO - [2021-06-18 10:30:18,629] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:30:18,643] {logging_mixin.py:104} INFO - [2021-06-18 10:30:18,643] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:30:18,650] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.055 seconds
[2021-06-18 10:35:42,458] {scheduler_job.py:182} INFO - Started process (PID=35) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:35:42,459] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:35:42,460] {logging_mixin.py:104} INFO - [2021-06-18 10:35:42,459] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:35:42,475] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:35:47,405] {logging_mixin.py:104} INFO - [2021-06-18 10:35:47,405] {manager.py:548} ERROR - Creation of Permission View Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "ab_permission_view_permission_id_view_menu_id_key"
DETAIL:  Key (permission_id, view_menu_id)=(1, 49) already exists.

[SQL: INSERT INTO ab_permission_view (id, permission_id, view_menu_id) VALUES (nextval('ab_permission_view_id_seq'), %(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 1, 'view_menu_id': 49}]
(Background on this error at: http://sqlalche.me/e/13/gkpj)
[2021-06-18 10:35:47,407] {logging_mixin.py:104} INFO - [2021-06-18 10:35:47,405] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:35:47,419] {logging_mixin.py:104} INFO - [2021-06-18 10:35:47,417] {dag.py:1851} INFO - Creating ORM DAG for etl_workflow
[2021-06-18 10:35:47,423] {logging_mixin.py:104} INFO - [2021-06-18 10:35:47,423] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:35:47,428] {scheduler_job.py:193} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(etl_workflow) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 187, in _run_file_processor
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 652, in process_file
    dagbag.sync_to_db()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 594, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.6/site-packages/tenacity/__init__.py", line 390, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.6/site-packages/tenacity/__init__.py", line 356, in iter
    return fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 608, in sync_to_db
    DAG.bulk_write_to_db(self.dags.values(), session=session)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 1919, in bulk_write_to_db
    DagCode.bulk_sync_to_db([dag.fileloc for dag in orm_dags])
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.6/contextlib.py", line 88, in __exit__
    next(self.gen)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 32, in create_session
    session.commit()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1046, in commit
    self.transaction.commit()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 504, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 483, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 1083, in _emit_insert_statements
    c = cached_connections[connection].execute(statement, multiparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(etl_workflow) already exists.

[SQL: INSERT INTO dag (dag_id, root_dag_id, is_paused, is_subdag, is_active, last_parsed_time, last_pickled, last_expired, scheduler_lock, pickle_id, fileloc, owners, description, default_view, schedule_interval, concurrency, has_task_concurrency_limits, next_dagrun, next_dagrun_create_after) VALUES (%(dag_id)s, %(root_dag_id)s, %(is_paused)s, %(is_subdag)s, %(is_active)s, %(last_parsed_time)s, %(last_pickled)s, %(last_expired)s, %(scheduler_lock)s, %(pickle_id)s, %(fileloc)s, %(owners)s, %(description)s, %(default_view)s, %(schedule_interval)s, %(concurrency)s, %(has_task_concurrency_limits)s, %(next_dagrun)s, %(next_dagrun_create_after)s)]
[parameters: {'dag_id': 'etl_workflow', 'root_dag_id': None, 'is_paused': True, 'is_subdag': False, 'is_active': True, 'last_parsed_time': datetime.datetime(2021, 6, 18, 10, 35, 47, 423620, tzinfo=Timezone('UTC')), 'last_pickled': None, 'last_expired': None, 'scheduler_lock': None, 'pickle_id': None, 'fileloc': '/opt/airflow/dags/etl.py', 'owners': '', 'description': None, 'default_view': 'tree', 'schedule_interval': '{"type": "timedelta", "attrs": {"days": 1, "seconds": 0, "microseconds": 0}}', 'concurrency': 16, 'has_task_concurrency_limits': False, 'next_dagrun': None, 'next_dagrun_create_after': None}]
(Background on this error at: http://sqlalche.me/e/13/gkpj)
[2021-06-18 10:36:18,090] {scheduler_job.py:182} INFO - Started process (PID=162) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:36:18,091] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:36:18,091] {logging_mixin.py:104} INFO - [2021-06-18 10:36:18,091] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:36:18,097] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:36:18,115] {logging_mixin.py:104} INFO - [2021-06-18 10:36:18,115] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:36:18,126] {logging_mixin.py:104} INFO - [2021-06-18 10:36:18,126] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:36:18,133] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.046 seconds
[2021-06-18 10:36:48,597] {scheduler_job.py:182} INFO - Started process (PID=237) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:36:48,598] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:36:48,599] {logging_mixin.py:104} INFO - [2021-06-18 10:36:48,599] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:36:48,605] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:36:48,784] {logging_mixin.py:104} INFO - [2021-06-18 10:36:48,784] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:36:48,799] {logging_mixin.py:104} INFO - [2021-06-18 10:36:48,799] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:36:48,807] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.214 seconds
[2021-06-18 10:37:18,910] {scheduler_job.py:182} INFO - Started process (PID=333) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:37:18,911] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:37:18,912] {logging_mixin.py:104} INFO - [2021-06-18 10:37:18,912] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:37:18,918] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:37:18,937] {logging_mixin.py:104} INFO - [2021-06-18 10:37:18,937] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:37:18,952] {logging_mixin.py:104} INFO - [2021-06-18 10:37:18,952] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:37:18,958] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.051 seconds
[2021-06-18 10:37:49,033] {scheduler_job.py:182} INFO - Started process (PID=429) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:37:49,034] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:37:49,035] {logging_mixin.py:104} INFO - [2021-06-18 10:37:49,034] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:37:49,042] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:37:49,202] {logging_mixin.py:104} INFO - [2021-06-18 10:37:49,202] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:37:49,215] {logging_mixin.py:104} INFO - [2021-06-18 10:37:49,215] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:37:49,225] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.196 seconds
[2021-06-18 10:38:19,467] {scheduler_job.py:182} INFO - Started process (PID=504) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:38:19,468] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:38:19,469] {logging_mixin.py:104} INFO - [2021-06-18 10:38:19,469] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:38:19,474] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:38:19,494] {logging_mixin.py:104} INFO - [2021-06-18 10:38:19,493] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:38:19,505] {logging_mixin.py:104} INFO - [2021-06-18 10:38:19,505] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:38:19,513] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.049 seconds
[2021-06-18 10:38:49,657] {scheduler_job.py:182} INFO - Started process (PID=602) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:38:49,658] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:38:49,658] {logging_mixin.py:104} INFO - [2021-06-18 10:38:49,658] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:38:49,664] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:38:49,831] {logging_mixin.py:104} INFO - [2021-06-18 10:38:49,831] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:38:49,843] {logging_mixin.py:104} INFO - [2021-06-18 10:38:49,842] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:38:49,852] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.199 seconds
[2021-06-18 10:39:20,271] {scheduler_job.py:182} INFO - Started process (PID=677) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:39:20,273] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:39:20,274] {logging_mixin.py:104} INFO - [2021-06-18 10:39:20,273] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:39:20,279] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:39:20,302] {logging_mixin.py:104} INFO - [2021-06-18 10:39:20,301] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:39:20,314] {logging_mixin.py:104} INFO - [2021-06-18 10:39:20,314] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:39:20,320] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.053 seconds
[2021-06-18 10:39:50,411] {scheduler_job.py:182} INFO - Started process (PID=775) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:39:50,411] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:39:50,412] {logging_mixin.py:104} INFO - [2021-06-18 10:39:50,412] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:39:50,419] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:39:50,601] {logging_mixin.py:104} INFO - [2021-06-18 10:39:50,600] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:39:50,615] {logging_mixin.py:104} INFO - [2021-06-18 10:39:50,615] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:39:50,625] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.217 seconds
[2021-06-18 10:40:20,818] {scheduler_job.py:182} INFO - Started process (PID=871) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:40:20,819] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:40:20,820] {logging_mixin.py:104} INFO - [2021-06-18 10:40:20,820] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:40:20,827] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:40:20,845] {logging_mixin.py:104} INFO - [2021-06-18 10:40:20,845] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:40:20,857] {logging_mixin.py:104} INFO - [2021-06-18 10:40:20,857] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:40:20,863] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.048 seconds
[2021-06-18 10:40:50,888] {scheduler_job.py:182} INFO - Started process (PID=945) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:40:50,889] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:40:50,890] {logging_mixin.py:104} INFO - [2021-06-18 10:40:50,890] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:40:50,899] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:40:51,076] {logging_mixin.py:104} INFO - [2021-06-18 10:40:51,076] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:40:51,089] {logging_mixin.py:104} INFO - [2021-06-18 10:40:51,089] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:40:51,099] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.215 seconds
[2021-06-18 10:41:21,135] {scheduler_job.py:182} INFO - Started process (PID=1044) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:41:21,136] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:41:21,137] {logging_mixin.py:104} INFO - [2021-06-18 10:41:21,137] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:41:21,142] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:41:21,160] {logging_mixin.py:104} INFO - [2021-06-18 10:41:21,160] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:41:21,172] {logging_mixin.py:104} INFO - [2021-06-18 10:41:21,172] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:41:21,181] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.049 seconds
[2021-06-18 10:41:51,620] {scheduler_job.py:182} INFO - Started process (PID=1119) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:41:51,621] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:41:51,622] {logging_mixin.py:104} INFO - [2021-06-18 10:41:51,622] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:41:51,627] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:41:51,797] {logging_mixin.py:104} INFO - [2021-06-18 10:41:51,797] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:41:51,809] {logging_mixin.py:104} INFO - [2021-06-18 10:41:51,809] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:41:51,818] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.202 seconds
[2021-06-18 10:42:21,880] {scheduler_job.py:182} INFO - Started process (PID=1215) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:42:21,882] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:42:21,883] {logging_mixin.py:104} INFO - [2021-06-18 10:42:21,883] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:42:21,888] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:42:21,908] {logging_mixin.py:104} INFO - [2021-06-18 10:42:21,908] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:42:21,921] {logging_mixin.py:104} INFO - [2021-06-18 10:42:21,921] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:42:21,927] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.051 seconds
[2021-06-18 10:42:51,981] {scheduler_job.py:182} INFO - Started process (PID=1311) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:42:51,983] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:42:51,983] {logging_mixin.py:104} INFO - [2021-06-18 10:42:51,983] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:42:51,988] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:42:52,149] {logging_mixin.py:104} INFO - [2021-06-18 10:42:52,149] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:42:52,163] {logging_mixin.py:104} INFO - [2021-06-18 10:42:52,163] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:42:52,172] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.195 seconds
[2021-06-18 10:43:22,453] {scheduler_job.py:182} INFO - Started process (PID=1387) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:43:22,454] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:43:22,455] {logging_mixin.py:104} INFO - [2021-06-18 10:43:22,455] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:43:22,461] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:43:22,479] {logging_mixin.py:104} INFO - [2021-06-18 10:43:22,479] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:43:22,492] {logging_mixin.py:104} INFO - [2021-06-18 10:43:22,492] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:43:22,500] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.051 seconds
[2021-06-18 10:43:52,815] {scheduler_job.py:182} INFO - Started process (PID=1483) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:43:52,816] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:43:52,816] {logging_mixin.py:104} INFO - [2021-06-18 10:43:52,816] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:43:52,822] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:43:52,987] {logging_mixin.py:104} INFO - [2021-06-18 10:43:52,987] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:43:52,998] {logging_mixin.py:104} INFO - [2021-06-18 10:43:52,998] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:43:53,008] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.195 seconds
[2021-06-18 10:44:23,145] {scheduler_job.py:182} INFO - Started process (PID=1561) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:44:23,146] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:44:23,146] {logging_mixin.py:104} INFO - [2021-06-18 10:44:23,146] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:44:23,154] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:44:23,192] {logging_mixin.py:104} INFO - [2021-06-18 10:44:23,192] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:44:23,209] {logging_mixin.py:104} INFO - [2021-06-18 10:44:23,209] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:44:23,216] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.074 seconds
[2021-06-18 10:44:53,228] {scheduler_job.py:182} INFO - Started process (PID=1657) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:44:53,229] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:44:53,229] {logging_mixin.py:104} INFO - [2021-06-18 10:44:53,229] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:44:53,235] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:44:53,398] {logging_mixin.py:104} INFO - [2021-06-18 10:44:53,398] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:44:53,410] {logging_mixin.py:104} INFO - [2021-06-18 10:44:53,410] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:44:53,421] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.194 seconds
[2021-06-18 10:45:23,625] {scheduler_job.py:182} INFO - Started process (PID=1753) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:45:23,626] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:45:23,626] {logging_mixin.py:104} INFO - [2021-06-18 10:45:23,626] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:45:23,633] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:45:23,650] {logging_mixin.py:104} INFO - [2021-06-18 10:45:23,649] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:45:23,663] {logging_mixin.py:104} INFO - [2021-06-18 10:45:23,663] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:45:23,671] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.049 seconds
[2021-06-18 10:45:53,819] {scheduler_job.py:182} INFO - Started process (PID=1826) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:45:53,821] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:45:53,821] {logging_mixin.py:104} INFO - [2021-06-18 10:45:53,821] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:45:53,828] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:45:53,996] {logging_mixin.py:104} INFO - [2021-06-18 10:45:53,996] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:45:54,008] {logging_mixin.py:104} INFO - [2021-06-18 10:45:54,008] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:45:54,017] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.201 seconds
[2021-06-18 10:46:24,130] {scheduler_job.py:182} INFO - Started process (PID=1922) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:46:24,131] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:46:24,132] {logging_mixin.py:104} INFO - [2021-06-18 10:46:24,132] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:46:24,137] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_workflow']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:46:24,152] {logging_mixin.py:104} INFO - [2021-06-18 10:46:24,152] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:46:24,163] {logging_mixin.py:104} INFO - [2021-06-18 10:46:24,163] {dag.py:2305} INFO - Setting next_dagrun for etl_workflow to None
[2021-06-18 10:46:24,170] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.044 seconds
[2021-06-18 10:46:54,410] {scheduler_job.py:182} INFO - Started process (PID=2006) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:46:54,412] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:46:54,412] {logging_mixin.py:104} INFO - [2021-06-18 10:46:54,412] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:46:54,419] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:46:56,199] {logging_mixin.py:104} INFO - [2021-06-18 10:46:56,197] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:46:56,211] {logging_mixin.py:104} INFO - [2021-06-18 10:46:56,211] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:46:56,217] {scheduler_job.py:193} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(etl_pipeline) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 187, in _run_file_processor
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 652, in process_file
    dagbag.sync_to_db()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 594, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.6/site-packages/tenacity/__init__.py", line 390, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.6/site-packages/tenacity/__init__.py", line 356, in iter
    return fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 608, in sync_to_db
    DAG.bulk_write_to_db(self.dags.values(), session=session)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 1919, in bulk_write_to_db
    DagCode.bulk_sync_to_db([dag.fileloc for dag in orm_dags])
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.6/contextlib.py", line 88, in __exit__
    next(self.gen)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 32, in create_session
    session.commit()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1046, in commit
    self.transaction.commit()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 504, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 483, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 1083, in _emit_insert_statements
    c = cached_connections[connection].execute(statement, multiparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(etl_pipeline) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, last_updated, dag_hash) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(last_updated)s, %(dag_hash)s)]
[parameters: {'dag_id': 'etl_pipeline', 'fileloc': '/opt/airflow/dags/etl.py', 'fileloc_hash': 34118401899435808, 'data': '{"__version": 1, "dag": {"default_args": {"__var": {"owner": "meng", "email": "darklemon2000@gmail.com", "start_date": {"__var": 1623974400.0, "__typ ... (277 characters truncated) ...  {}, "upstream_group_ids": [], "downstream_group_ids": [], "upstream_task_ids": [], "downstream_task_ids": []}, "tasks": [], "dag_dependencies": []}}', 'last_updated': datetime.datetime(2021, 6, 18, 10, 46, 54, 439136, tzinfo=Timezone('UTC')), 'dag_hash': '64d37aee0ecfc1bed7db9722063b7dd8'}]
(Background on this error at: http://sqlalche.me/e/13/gkpj)
[2021-06-18 10:47:26,790] {scheduler_job.py:182} INFO - Started process (PID=2096) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:47:26,791] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:47:26,791] {logging_mixin.py:104} INFO - [2021-06-18 10:47:26,791] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:47:26,796] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:47:26,977] {logging_mixin.py:104} INFO - [2021-06-18 10:47:26,977] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:47:26,992] {logging_mixin.py:104} INFO - [2021-06-18 10:47:26,992] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:47:27,006] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.218 seconds
[2021-06-18 10:47:57,041] {scheduler_job.py:182} INFO - Started process (PID=2192) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:47:57,044] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:47:57,044] {logging_mixin.py:104} INFO - [2021-06-18 10:47:57,044] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:47:57,050] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:47:57,066] {logging_mixin.py:104} INFO - [2021-06-18 10:47:57,065] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:47:57,080] {logging_mixin.py:104} INFO - [2021-06-18 10:47:57,080] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:47:57,086] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.049 seconds
[2021-06-18 10:48:27,436] {scheduler_job.py:182} INFO - Started process (PID=2267) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:48:27,437] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:48:27,438] {logging_mixin.py:104} INFO - [2021-06-18 10:48:27,438] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:48:27,445] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:48:27,634] {logging_mixin.py:104} INFO - [2021-06-18 10:48:27,634] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:48:27,648] {logging_mixin.py:104} INFO - [2021-06-18 10:48:27,648] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:48:27,659] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.228 seconds
[2021-06-18 10:48:57,778] {scheduler_job.py:182} INFO - Started process (PID=2363) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:48:57,779] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:48:57,780] {logging_mixin.py:104} INFO - [2021-06-18 10:48:57,780] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:48:57,785] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:48:57,806] {logging_mixin.py:104} INFO - [2021-06-18 10:48:57,806] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:48:57,818] {logging_mixin.py:104} INFO - [2021-06-18 10:48:57,818] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:48:57,825] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.052 seconds
[2021-06-18 10:49:28,048] {scheduler_job.py:182} INFO - Started process (PID=2459) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:49:28,050] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:49:28,051] {logging_mixin.py:104} INFO - [2021-06-18 10:49:28,050] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:49:28,057] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:49:28,238] {logging_mixin.py:104} INFO - [2021-06-18 10:49:28,238] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:49:28,252] {logging_mixin.py:104} INFO - [2021-06-18 10:49:28,251] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:49:28,261] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.217 seconds
[2021-06-18 10:49:58,335] {scheduler_job.py:182} INFO - Started process (PID=2535) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:49:58,336] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:49:58,337] {logging_mixin.py:104} INFO - [2021-06-18 10:49:58,337] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:49:58,342] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:49:58,364] {logging_mixin.py:104} INFO - [2021-06-18 10:49:58,363] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:49:58,376] {logging_mixin.py:104} INFO - [2021-06-18 10:49:58,376] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:49:58,384] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.053 seconds
[2021-06-18 10:50:28,754] {scheduler_job.py:182} INFO - Started process (PID=2631) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:50:28,756] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:50:28,757] {logging_mixin.py:104} INFO - [2021-06-18 10:50:28,756] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:50:28,763] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:50:28,927] {logging_mixin.py:104} INFO - [2021-06-18 10:50:28,927] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:50:28,939] {logging_mixin.py:104} INFO - [2021-06-18 10:50:28,939] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:50:28,945] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.195 seconds
[2021-06-18 10:50:59,119] {scheduler_job.py:182} INFO - Started process (PID=2716) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:50:59,121] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:50:59,121] {logging_mixin.py:104} INFO - [2021-06-18 10:50:59,121] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:50:59,128] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:50:59,145] {logging_mixin.py:104} INFO - [2021-06-18 10:50:59,145] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:50:59,161] {logging_mixin.py:104} INFO - [2021-06-18 10:50:59,161] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:50:59,168] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.053 seconds
[2021-06-18 10:51:29,458] {scheduler_job.py:182} INFO - Started process (PID=2803) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:51:29,459] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:51:29,460] {logging_mixin.py:104} INFO - [2021-06-18 10:51:29,460] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:51:29,467] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:51:29,627] {logging_mixin.py:104} INFO - [2021-06-18 10:51:29,627] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:51:29,640] {logging_mixin.py:104} INFO - [2021-06-18 10:51:29,640] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:51:29,647] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.192 seconds
[2021-06-18 10:51:59,666] {scheduler_job.py:182} INFO - Started process (PID=2899) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:51:59,667] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:51:59,667] {logging_mixin.py:104} INFO - [2021-06-18 10:51:59,667] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:51:59,674] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:51:59,695] {logging_mixin.py:104} INFO - [2021-06-18 10:51:59,695] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:51:59,711] {logging_mixin.py:104} INFO - [2021-06-18 10:51:59,711] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:51:59,719] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.056 seconds
[2021-06-18 10:52:30,083] {scheduler_job.py:182} INFO - Started process (PID=2974) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:52:30,084] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:52:30,084] {logging_mixin.py:104} INFO - [2021-06-18 10:52:30,084] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:52:30,090] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:52:30,266] {logging_mixin.py:104} INFO - [2021-06-18 10:52:30,266] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:52:30,280] {logging_mixin.py:104} INFO - [2021-06-18 10:52:30,280] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:52:30,291] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.212 seconds
[2021-06-18 10:53:00,410] {scheduler_job.py:182} INFO - Started process (PID=3070) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:53:00,411] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:53:00,411] {logging_mixin.py:104} INFO - [2021-06-18 10:53:00,411] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:53:00,417] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:53:00,434] {logging_mixin.py:104} INFO - [2021-06-18 10:53:00,434] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:53:00,448] {logging_mixin.py:104} INFO - [2021-06-18 10:53:00,448] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:53:00,453] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.047 seconds
[2021-06-18 10:53:30,741] {scheduler_job.py:182} INFO - Started process (PID=3166) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:53:30,742] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:53:30,742] {logging_mixin.py:104} INFO - [2021-06-18 10:53:30,742] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:53:30,748] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:53:30,940] {logging_mixin.py:104} INFO - [2021-06-18 10:53:30,940] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:53:30,953] {logging_mixin.py:104} INFO - [2021-06-18 10:53:30,953] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:53:30,960] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.221 seconds
[2021-06-18 10:54:00,987] {scheduler_job.py:182} INFO - Started process (PID=3242) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:54:00,988] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:54:00,989] {logging_mixin.py:104} INFO - [2021-06-18 10:54:00,989] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:54:00,996] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:54:01,015] {logging_mixin.py:104} INFO - [2021-06-18 10:54:01,015] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:54:01,029] {logging_mixin.py:104} INFO - [2021-06-18 10:54:01,029] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:54:01,035] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.052 seconds
[2021-06-18 10:54:31,305] {scheduler_job.py:182} INFO - Started process (PID=3339) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:54:31,307] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:54:31,308] {logging_mixin.py:104} INFO - [2021-06-18 10:54:31,308] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:54:31,314] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:54:31,491] {logging_mixin.py:104} INFO - [2021-06-18 10:54:31,491] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:54:31,501] {logging_mixin.py:104} INFO - [2021-06-18 10:54:31,501] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:54:31,508] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.207 seconds
[2021-06-18 10:55:01,657] {scheduler_job.py:182} INFO - Started process (PID=3417) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:55:01,658] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:55:01,658] {logging_mixin.py:104} INFO - [2021-06-18 10:55:01,658] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:55:01,665] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:55:01,689] {logging_mixin.py:104} INFO - [2021-06-18 10:55:01,689] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:55:01,701] {logging_mixin.py:104} INFO - [2021-06-18 10:55:01,701] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:55:01,708] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.054 seconds
[2021-06-18 10:55:31,762] {scheduler_job.py:182} INFO - Started process (PID=3510) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:55:31,763] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:55:31,764] {logging_mixin.py:104} INFO - [2021-06-18 10:55:31,764] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:55:31,769] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:55:31,930] {logging_mixin.py:104} INFO - [2021-06-18 10:55:31,930] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:55:31,943] {logging_mixin.py:104} INFO - [2021-06-18 10:55:31,943] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:55:31,949] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.191 seconds
[2021-06-18 10:56:02,099] {scheduler_job.py:182} INFO - Started process (PID=3606) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:56:02,100] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:56:02,101] {logging_mixin.py:104} INFO - [2021-06-18 10:56:02,101] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:56:02,106] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:56:02,125] {logging_mixin.py:104} INFO - [2021-06-18 10:56:02,125] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:56:02,145] {logging_mixin.py:104} INFO - [2021-06-18 10:56:02,145] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:56:02,152] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.056 seconds
[2021-06-18 10:56:32,387] {scheduler_job.py:182} INFO - Started process (PID=3681) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:56:32,389] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:56:32,389] {logging_mixin.py:104} INFO - [2021-06-18 10:56:32,389] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:56:32,396] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:56:32,564] {logging_mixin.py:104} INFO - [2021-06-18 10:56:32,564] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:56:32,576] {logging_mixin.py:104} INFO - [2021-06-18 10:56:32,576] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:56:32,586] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.202 seconds
[2021-06-18 10:57:02,748] {scheduler_job.py:182} INFO - Started process (PID=3777) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:57:02,750] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:57:02,751] {logging_mixin.py:104} INFO - [2021-06-18 10:57:02,750] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:57:02,761] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:57:02,778] {logging_mixin.py:104} INFO - [2021-06-18 10:57:02,778] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:57:02,793] {logging_mixin.py:104} INFO - [2021-06-18 10:57:02,792] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:57:02,798] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.052 seconds
[2021-06-18 10:57:33,011] {scheduler_job.py:182} INFO - Started process (PID=3853) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:57:33,013] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:57:33,013] {logging_mixin.py:104} INFO - [2021-06-18 10:57:33,013] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:57:33,019] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:57:33,180] {logging_mixin.py:104} INFO - [2021-06-18 10:57:33,180] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:57:33,192] {logging_mixin.py:104} INFO - [2021-06-18 10:57:33,192] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:57:33,200] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.192 seconds
[2021-06-18 10:58:03,327] {scheduler_job.py:182} INFO - Started process (PID=3950) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:58:03,329] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:58:03,329] {logging_mixin.py:104} INFO - [2021-06-18 10:58:03,329] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:58:03,333] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:58:03,349] {logging_mixin.py:104} INFO - [2021-06-18 10:58:03,348] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:58:03,366] {logging_mixin.py:104} INFO - [2021-06-18 10:58:03,366] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:58:03,374] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.051 seconds
[2021-06-18 10:58:33,481] {scheduler_job.py:182} INFO - Started process (PID=4045) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:58:33,482] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:58:33,483] {logging_mixin.py:104} INFO - [2021-06-18 10:58:33,483] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:58:33,489] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:58:33,652] {logging_mixin.py:104} INFO - [2021-06-18 10:58:33,652] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:58:33,663] {logging_mixin.py:104} INFO - [2021-06-18 10:58:33,662] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:58:33,671] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.194 seconds
[2021-06-18 10:59:03,798] {scheduler_job.py:182} INFO - Started process (PID=4120) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:59:03,799] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:59:03,800] {logging_mixin.py:104} INFO - [2021-06-18 10:59:03,800] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:59:03,807] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:59:03,831] {logging_mixin.py:104} INFO - [2021-06-18 10:59:03,830] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:59:03,844] {logging_mixin.py:104} INFO - [2021-06-18 10:59:03,844] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:59:03,851] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.056 seconds
[2021-06-18 10:59:34,117] {scheduler_job.py:182} INFO - Started process (PID=4217) to work on /opt/airflow/dags/etl.py
[2021-06-18 10:59:34,119] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 10:59:34,120] {logging_mixin.py:104} INFO - [2021-06-18 10:59:34,120] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 10:59:34,127] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 10:59:34,302] {logging_mixin.py:104} INFO - [2021-06-18 10:59:34,301] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 10:59:34,317] {logging_mixin.py:104} INFO - [2021-06-18 10:59:34,317] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 10:59:34,325] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.212 seconds
[2021-06-18 11:00:04,458] {scheduler_job.py:182} INFO - Started process (PID=4292) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:00:04,459] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:00:04,459] {logging_mixin.py:104} INFO - [2021-06-18 11:00:04,459] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:00:04,467] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:00:04,485] {logging_mixin.py:104} INFO - [2021-06-18 11:00:04,485] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:00:04,501] {logging_mixin.py:104} INFO - [2021-06-18 11:00:04,501] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:00:04,509] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.055 seconds
[2021-06-18 11:00:34,667] {scheduler_job.py:182} INFO - Started process (PID=4388) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:00:34,668] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:00:34,669] {logging_mixin.py:104} INFO - [2021-06-18 11:00:34,669] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:00:34,675] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:00:34,842] {logging_mixin.py:104} INFO - [2021-06-18 11:00:34,842] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:00:34,854] {logging_mixin.py:104} INFO - [2021-06-18 11:00:34,854] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:00:34,862] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.199 seconds
[2021-06-18 11:01:04,955] {scheduler_job.py:182} INFO - Started process (PID=4485) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:01:04,957] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:01:04,957] {logging_mixin.py:104} INFO - [2021-06-18 11:01:04,957] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:01:04,963] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:01:04,980] {logging_mixin.py:104} INFO - [2021-06-18 11:01:04,980] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:01:04,995] {logging_mixin.py:104} INFO - [2021-06-18 11:01:04,994] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:01:05,002] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.051 seconds
[2021-06-18 11:01:35,237] {scheduler_job.py:182} INFO - Started process (PID=4560) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:01:35,238] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:01:35,238] {logging_mixin.py:104} INFO - [2021-06-18 11:01:35,238] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:01:35,244] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:01:35,413] {logging_mixin.py:104} INFO - [2021-06-18 11:01:35,413] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:01:35,425] {logging_mixin.py:104} INFO - [2021-06-18 11:01:35,425] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:01:35,433] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.200 seconds
[2021-06-18 11:02:05,655] {scheduler_job.py:182} INFO - Started process (PID=4656) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:02:05,657] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:02:05,657] {logging_mixin.py:104} INFO - [2021-06-18 11:02:05,657] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:02:05,666] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:02:05,683] {logging_mixin.py:104} INFO - [2021-06-18 11:02:05,682] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:02:05,696] {logging_mixin.py:104} INFO - [2021-06-18 11:02:05,696] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:02:05,702] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.050 seconds
[2021-06-18 11:02:36,114] {scheduler_job.py:182} INFO - Started process (PID=4738) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:02:36,115] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:02:36,115] {logging_mixin.py:104} INFO - [2021-06-18 11:02:36,115] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:02:36,122] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:02:36,284] {logging_mixin.py:104} INFO - [2021-06-18 11:02:36,284] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:02:36,296] {logging_mixin.py:104} INFO - [2021-06-18 11:02:36,296] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:02:36,303] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.193 seconds
[2021-06-18 11:03:06,353] {scheduler_job.py:182} INFO - Started process (PID=4829) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:03:06,354] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:03:06,355] {logging_mixin.py:104} INFO - [2021-06-18 11:03:06,355] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:03:06,361] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:03:06,541] {logging_mixin.py:104} INFO - [2021-06-18 11:03:06,540] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:03:06,554] {logging_mixin.py:104} INFO - [2021-06-18 11:03:06,554] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:03:06,570] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.219 seconds
[2021-06-18 11:03:36,731] {scheduler_job.py:182} INFO - Started process (PID=4923) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:03:36,733] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:03:36,733] {logging_mixin.py:104} INFO - [2021-06-18 11:03:36,733] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:03:36,738] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:03:36,757] {logging_mixin.py:104} INFO - [2021-06-18 11:03:36,757] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:03:36,770] {logging_mixin.py:104} INFO - [2021-06-18 11:03:36,770] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:03:36,778] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.050 seconds
[2021-06-18 11:04:07,094] {scheduler_job.py:182} INFO - Started process (PID=4998) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:04:07,095] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:04:07,095] {logging_mixin.py:104} INFO - [2021-06-18 11:04:07,095] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:04:07,100] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:04:07,262] {logging_mixin.py:104} INFO - [2021-06-18 11:04:07,262] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:04:07,274] {logging_mixin.py:104} INFO - [2021-06-18 11:04:07,274] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:04:07,283] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.194 seconds
[2021-06-18 11:04:37,400] {scheduler_job.py:182} INFO - Started process (PID=5097) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:04:37,401] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:04:37,402] {logging_mixin.py:104} INFO - [2021-06-18 11:04:37,402] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:04:37,408] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:04:37,427] {logging_mixin.py:104} INFO - [2021-06-18 11:04:37,427] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:04:37,446] {logging_mixin.py:104} INFO - [2021-06-18 11:04:37,446] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:04:37,453] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.058 seconds
[2021-06-18 11:05:07,523] {scheduler_job.py:182} INFO - Started process (PID=5181) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:05:07,524] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:05:07,525] {logging_mixin.py:104} INFO - [2021-06-18 11:05:07,525] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:05:07,531] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:05:07,700] {logging_mixin.py:104} INFO - [2021-06-18 11:05:07,700] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:05:07,713] {logging_mixin.py:104} INFO - [2021-06-18 11:05:07,713] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:05:07,722] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.203 seconds
[2021-06-18 11:05:37,776] {scheduler_job.py:182} INFO - Started process (PID=5267) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:05:37,777] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:05:37,778] {logging_mixin.py:104} INFO - [2021-06-18 11:05:37,777] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:05:37,784] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:05:37,804] {logging_mixin.py:104} INFO - [2021-06-18 11:05:37,804] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:05:37,817] {logging_mixin.py:104} INFO - [2021-06-18 11:05:37,817] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:05:37,824] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.051 seconds
[2021-06-18 11:06:08,008] {scheduler_job.py:182} INFO - Started process (PID=5361) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:06:08,009] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:06:08,010] {logging_mixin.py:104} INFO - [2021-06-18 11:06:08,010] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:06:08,016] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:06:08,196] {logging_mixin.py:104} INFO - [2021-06-18 11:06:08,196] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:06:08,276] {logging_mixin.py:104} INFO - [2021-06-18 11:06:08,276] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:06:08,284] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.280 seconds
[2021-06-18 11:06:38,628] {scheduler_job.py:182} INFO - Started process (PID=5436) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:06:38,629] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:06:38,630] {logging_mixin.py:104} INFO - [2021-06-18 11:06:38,629] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:06:38,636] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:06:38,653] {logging_mixin.py:104} INFO - [2021-06-18 11:06:38,653] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:06:38,665] {logging_mixin.py:104} INFO - [2021-06-18 11:06:38,665] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:06:38,672] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.047 seconds
[2021-06-18 11:07:08,927] {scheduler_job.py:182} INFO - Started process (PID=5532) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:07:08,929] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:07:08,929] {logging_mixin.py:104} INFO - [2021-06-18 11:07:08,929] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:07:08,934] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:07:09,096] {logging_mixin.py:104} INFO - [2021-06-18 11:07:09,096] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:07:09,175] {logging_mixin.py:104} INFO - [2021-06-18 11:07:09,175] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:07:09,181] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.258 seconds
[2021-06-18 11:07:39,231] {scheduler_job.py:182} INFO - Started process (PID=5631) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:07:39,232] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:07:39,233] {logging_mixin.py:104} INFO - [2021-06-18 11:07:39,233] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:07:39,239] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:07:39,255] {logging_mixin.py:104} INFO - [2021-06-18 11:07:39,255] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:07:39,271] {logging_mixin.py:104} INFO - [2021-06-18 11:07:39,270] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:07:39,279] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.050 seconds
[2021-06-18 11:08:09,413] {scheduler_job.py:182} INFO - Started process (PID=5703) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:08:09,414] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:08:09,415] {logging_mixin.py:104} INFO - [2021-06-18 11:08:09,415] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:08:09,422] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:08:09,583] {logging_mixin.py:104} INFO - [2021-06-18 11:08:09,583] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:08:09,663] {logging_mixin.py:104} INFO - [2021-06-18 11:08:09,663] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:08:09,670] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.261 seconds
[2021-06-18 11:08:39,819] {scheduler_job.py:182} INFO - Started process (PID=5802) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:08:39,820] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:08:39,820] {logging_mixin.py:104} INFO - [2021-06-18 11:08:39,820] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:08:39,826] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:08:39,845] {logging_mixin.py:104} INFO - [2021-06-18 11:08:39,845] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:08:39,858] {logging_mixin.py:104} INFO - [2021-06-18 11:08:39,858] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:08:39,867] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.050 seconds
[2021-06-18 11:09:10,095] {scheduler_job.py:182} INFO - Started process (PID=5874) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:09:10,096] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:09:10,097] {logging_mixin.py:104} INFO - [2021-06-18 11:09:10,097] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:09:10,103] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:09:10,299] {logging_mixin.py:104} INFO - [2021-06-18 11:09:10,299] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:09:10,384] {logging_mixin.py:104} INFO - [2021-06-18 11:09:10,384] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:09:10,392] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.300 seconds
[2021-06-18 11:09:40,491] {scheduler_job.py:182} INFO - Started process (PID=5973) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:09:40,491] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:09:40,492] {logging_mixin.py:104} INFO - [2021-06-18 11:09:40,492] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:09:40,499] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:09:40,517] {logging_mixin.py:104} INFO - [2021-06-18 11:09:40,517] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:09:40,530] {logging_mixin.py:104} INFO - [2021-06-18 11:09:40,530] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:09:40,537] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.049 seconds
[2021-06-18 11:10:10,862] {scheduler_job.py:182} INFO - Started process (PID=6066) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:10:10,864] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:10:10,865] {logging_mixin.py:104} INFO - [2021-06-18 11:10:10,864] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:10:10,870] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:10:11,031] {logging_mixin.py:104} INFO - [2021-06-18 11:10:11,031] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:10:11,106] {logging_mixin.py:104} INFO - [2021-06-18 11:10:11,106] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:10:11,115] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.257 seconds
[2021-06-18 11:10:41,302] {scheduler_job.py:182} INFO - Started process (PID=6141) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:10:41,303] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:10:41,304] {logging_mixin.py:104} INFO - [2021-06-18 11:10:41,304] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:10:41,310] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:10:41,328] {logging_mixin.py:104} INFO - [2021-06-18 11:10:41,327] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:10:41,340] {logging_mixin.py:104} INFO - [2021-06-18 11:10:41,340] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:10:41,345] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.048 seconds
[2021-06-18 11:11:11,869] {scheduler_job.py:182} INFO - Started process (PID=6237) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:11:11,870] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:11:11,870] {logging_mixin.py:104} INFO - [2021-06-18 11:11:11,870] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:11:11,876] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:11:12,111] {logging_mixin.py:104} INFO - [2021-06-18 11:11:12,111] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:11:12,121] {logging_mixin.py:104} INFO - [2021-06-18 11:11:12,121] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:11:12,130] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.265 seconds
[2021-06-18 11:11:42,390] {scheduler_job.py:182} INFO - Started process (PID=6323) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:11:42,391] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:11:42,392] {logging_mixin.py:104} INFO - [2021-06-18 11:11:42,392] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:11:42,399] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:11:42,419] {logging_mixin.py:104} INFO - [2021-06-18 11:11:42,419] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:11:42,433] {logging_mixin.py:104} INFO - [2021-06-18 11:11:42,433] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:11:42,439] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.054 seconds
[2021-06-18 11:12:12,842] {scheduler_job.py:182} INFO - Started process (PID=6409) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:12:12,843] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:12:12,844] {logging_mixin.py:104} INFO - [2021-06-18 11:12:12,844] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:12:12,850] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:12:13,095] {logging_mixin.py:104} INFO - [2021-06-18 11:12:13,095] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:12:13,105] {logging_mixin.py:104} INFO - [2021-06-18 11:12:13,105] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:12:13,114] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.274 seconds
[2021-06-18 11:12:43,222] {scheduler_job.py:182} INFO - Started process (PID=6506) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:12:43,223] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:12:43,224] {logging_mixin.py:104} INFO - [2021-06-18 11:12:43,224] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:12:43,229] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_pipeline']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:12:43,244] {logging_mixin.py:104} INFO - [2021-06-18 11:12:43,244] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:12:43,257] {logging_mixin.py:104} INFO - [2021-06-18 11:12:43,257] {dag.py:2305} INFO - Setting next_dagrun for etl_pipeline to None
[2021-06-18 11:12:43,262] {scheduler_job.py:190} INFO - Processing /opt/airflow/dags/etl.py took 0.044 seconds
[2021-06-18 11:13:13,617] {scheduler_job.py:182} INFO - Started process (PID=6581) to work on /opt/airflow/dags/etl.py
[2021-06-18 11:13:13,619] {scheduler_job.py:633} INFO - Processing file /opt/airflow/dags/etl.py for tasks to queue
[2021-06-18 11:13:13,620] {logging_mixin.py:104} INFO - [2021-06-18 11:13:13,619] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/dags/etl.py
[2021-06-18 11:13:13,625] {scheduler_job.py:643} INFO - DAG(s) dict_keys(['etl_example']) retrieved from /opt/airflow/dags/etl.py
[2021-06-18 11:13:15,300] {logging_mixin.py:104} INFO - [2021-06-18 11:13:15,300] {manager.py:548} ERROR - Creation of Permission View Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "ab_permission_view_permission_id_view_menu_id_key"
DETAIL:  Key (permission_id, view_menu_id)=(1, 83) already exists.

[SQL: INSERT INTO ab_permission_view (id, permission_id, view_menu_id) VALUES (nextval('ab_permission_view_id_seq'), %(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 1, 'view_menu_id': 83}]
(Background on this error at: http://sqlalche.me/e/13/gkpj)
[2021-06-18 11:13:15,302] {logging_mixin.py:104} INFO - [2021-06-18 11:13:15,300] {dag.py:1832} INFO - Sync 1 DAGs
[2021-06-18 11:13:15,313] {logging_mixin.py:104} INFO - [2021-06-18 11:13:15,311] {dag.py:1851} INFO - Creating ORM DAG for etl_example
[2021-06-18 11:13:15,316] {logging_mixin.py:104} INFO - [2021-06-18 11:13:15,316] {dag.py:2305} INFO - Setting next_dagrun for etl_example to None
[2021-06-18 11:13:15,322] {scheduler_job.py:193} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(etl_example) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 187, in _run_file_processor
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 652, in process_file
    dagbag.sync_to_db()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 594, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.6/site-packages/tenacity/__init__.py", line 390, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.6/site-packages/tenacity/__init__.py", line 356, in iter
    return fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 608, in sync_to_db
    DAG.bulk_write_to_db(self.dags.values(), session=session)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 1919, in bulk_write_to_db
    DagCode.bulk_sync_to_db([dag.fileloc for dag in orm_dags])
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.6/contextlib.py", line 88, in __exit__
    next(self.gen)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 32, in create_session
    session.commit()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1046, in commit
    self.transaction.commit()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 504, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 483, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 1083, in _emit_insert_statements
    c = cached_connections[connection].execute(statement, multiparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(etl_example) already exists.

[SQL: INSERT INTO dag (dag_id, root_dag_id, is_paused, is_subdag, is_active, last_parsed_time, last_pickled, last_expired, scheduler_lock, pickle_id, fileloc, owners, description, default_view, schedule_interval, concurrency, has_task_concurrency_limits, next_dagrun, next_dagrun_create_after) VALUES (%(dag_id)s, %(root_dag_id)s, %(is_paused)s, %(is_subdag)s, %(is_active)s, %(last_parsed_time)s, %(last_pickled)s, %(last_expired)s, %(scheduler_lock)s, %(pickle_id)s, %(fileloc)s, %(owners)s, %(description)s, %(default_view)s, %(schedule_interval)s, %(concurrency)s, %(has_task_concurrency_limits)s, %(next_dagrun)s, %(next_dagrun_create_after)s)]
[parameters: {'dag_id': 'etl_example', 'root_dag_id': None, 'is_paused': True, 'is_subdag': False, 'is_active': True, 'last_parsed_time': datetime.datetime(2021, 6, 18, 11, 13, 15, 316848, tzinfo=Timezone('UTC')), 'last_pickled': None, 'last_expired': None, 'scheduler_lock': None, 'pickle_id': None, 'fileloc': '/opt/airflow/dags/etl.py', 'owners': '', 'description': None, 'default_view': 'tree', 'schedule_interval': '{"type": "timedelta", "attrs": {"days": 1, "seconds": 0, "microseconds": 0}}', 'concurrency': 16, 'has_task_concurrency_limits': False, 'next_dagrun': None, 'next_dagrun_create_after': None}]
(Background on this error at: http://sqlalche.me/e/13/gkpj)
